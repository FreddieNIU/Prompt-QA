{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d389cc86-14f2-4ab9-bdee-27e88e11e3da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "# import svgling\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download([\n",
    "    \"names\",\n",
    "    \"stopwords\",\n",
    "    \"state_union\",\n",
    "    \"twitter_samples\",\n",
    "    \"movie_reviews\",\n",
    "    \"averaged_perceptron_tagger\",\n",
    "    \"vader_lexicon\",\n",
    "    \"punkt\",\n",
    "    \"maxent_ne_chunker\",\n",
    "    \"words\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f2c3ff-6dca-407d-af9e-acca81c3c226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testset0 = json.load(open('../../data/NewsQA/testsets/newsQA_test_0.json', 'r', encoding='utf-8'))\n",
    "testset1 = json.load(open('../../data/NewsQA/testsets/newsQA_test_1.json', 'r', encoding='utf-8'))\n",
    "testset2 = json.load(open('../../data/NewsQA/testsets/newsQA_test_2.json', 'r', encoding='utf-8'))\n",
    "testset3 = json.load(open('../../data/NewsQA/testsets/newsQA_test_3.json', 'r', encoding='utf-8'))\n",
    "testset4 = json.load(open('../../data/NewsQA/testsets/newsQA_test_4.json', 'r', encoding='utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b7e06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'NEW DELHI, India (CNN) -- A high court in northern India on Friday acquitted a wealthy businessman facing the death sentence for the killing of a teen in a case dubbed \"the house of horrors.\"\\n\\n\\n\\nMoninder Singh Pandher was sentenced to death by a lower court in February.\\n\\n\\n\\nThe teen was one of 19 victims -- children and young women -- in one of the most gruesome serial killings in India in recent years.\\n\\n\\n\\nThe Allahabad high court has acquitted Moninder Singh Pandher, his lawyer Sikandar B. Kochar told CNN.\\n\\n\\n\\nPandher and his domestic employee Surinder Koli were sentenced to death in February by a lower court for the rape and murder of the 14-year-old.\\n\\n\\n\\nThe high court upheld Koli\\'s death sentence, Kochar said.\\n\\n\\n\\nThe two were arrested two years ago after body parts packed in plastic bags were found near their home in Noida, a New Delhi suburb. Their home was later dubbed a \"house of horrors\" by the Indian media.\\n\\n\\n\\nPandher was not named a main suspect by investigators initially, but was summoned as co-accused during the trial, Kochar said.\\n\\n\\n\\nKochar said his client was in Australia when the teen was raped and killed.\\n\\n\\n\\nPandher faces trial in the remaining 18 killings and could remain in custody, the attorney said.',\n",
       " 'question': 'What was the amount of children murdered?',\n",
       " 'answer': [294, 298],\n",
       " 'answer_text': '19 ',\n",
       " 'start_position': 53,\n",
       " 'end_position': 53}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qa_dataset = testset0+testset1+testset2+testset3+testset4\n",
    "qa_dataset = json.load(open('../../data/NewsQA/processed_newsQA.json', 'r', encoding='utf-8'))\n",
    "qa_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d9d19",
   "metadata": {},
   "source": [
    "### Question Type Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e91c258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of qa pairs:  31423\n"
     ]
    }
   ],
   "source": [
    "print('Number of qa pairs: ', len(qa_dataset))\n",
    "question_list = []\n",
    "for qa in qa_dataset: \n",
    "    question_list.append(qa['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3d4caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why:  28 What:  14528 Where 2523 When 1369 How 279 How many 1741 How much 303 Which 770 Who 7201 Other 2681\n",
      "31423\n",
      "31423\n"
     ]
    }
   ],
   "source": [
    "what = {'count':0, 'list':[]}\n",
    "why = {'count':0, 'list':[]}\n",
    "where = {'count':0, 'list':[]}\n",
    "when = {'count':0, 'list':[]}\n",
    "how = {'count':0, 'list':[]}\n",
    "how_much = {'count':0, 'list':[]}\n",
    "how_many = {'count':0, 'list':[]}\n",
    "which = {'count':0, 'list':[]}\n",
    "who = {'count':0, 'list':[]}\n",
    "other = {'count':0, 'list':[]}\n",
    "\n",
    "for query in question_list:\n",
    "    query = query.capitalize()\n",
    "    if 'What' in query:\n",
    "        what['count'] += 1\n",
    "        what['list'].append(query)\n",
    "    elif 'Why' in query:\n",
    "        why['count'] += 1\n",
    "        why['list'].append(query)\n",
    "    elif 'Where' in query:\n",
    "        where['count'] += 1\n",
    "        where['list'].append(query)\n",
    "    elif 'When' in query:\n",
    "        when['count'] += 1\n",
    "        when['list'].append(query)\n",
    "    elif 'How much' in query:\n",
    "        how_much['count'] += 1\n",
    "        how_much['list'].append(query)\n",
    "    elif 'How many' in query:\n",
    "        how_many['count'] += 1\n",
    "        how_many['list'].append(query)\n",
    "    elif 'How' in query:\n",
    "        how['count'] += 1\n",
    "        how['list'].append(query)\n",
    "    elif 'Which' in query:\n",
    "        which['count'] += 1\n",
    "        which['list'].append(query)\n",
    "    elif 'Who' in query:\n",
    "        who['count'] += 1\n",
    "        who['list'].append(query)\n",
    "    else:\n",
    "        other['count'] += 1\n",
    "        other['list'].append(query)\n",
    "print('Why: ',why['count'], 'What: ',what['count'], 'Where',where['count'], 'When',when['count'], 'How',how['count'], 'How many', how_many['count'], 'How much', how_much['count'], 'Which',which['count'],'Who',who['count'], 'Other', other['count'])\n",
    "print(why['count']+what['count']+where['count']+when['count']+how['count']+how_much['count']+how_many['count']+which['count']+who['count']+other['count'])\n",
    "print(len(question_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d919086f",
   "metadata": {},
   "source": [
    "### Add Qtype template to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc5b8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(dataset):\n",
    "    words = {}\n",
    "    for i in range(len(dataset)):\n",
    "        for word in dataset[i].split():\n",
    "            if word not in words:\n",
    "                words[word]=1\n",
    "            else:\n",
    "                words[word]+=1\n",
    "    return words\n",
    "\n",
    "def sort_wordCounter(words_dict):\n",
    "    # input: a dict of words, key:word, value:times of word shown in the context\n",
    "    # output: a list sorted by the value of dict\n",
    "    a = dict(sorted(words_dict.items(), key=lambda x:x[1], reverse=True))\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8406d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "what = {'count':0, 'list':[]}\n",
    "why = {'count':0, 'list':[]}\n",
    "where = {'count':0, 'list':[]}\n",
    "when = {'count':0, 'list':[]}\n",
    "how = {'count':0, 'list':[]}\n",
    "how_much = {'count':0, 'list':[]}\n",
    "how_many = {'count':0, 'list':[]}\n",
    "which = {'count':0, 'list':[]}\n",
    "who = {'count':0, 'list':[]}\n",
    "other = {'count':0, 'list':[]}\n",
    "\n",
    "for qa in qa_dataset:\n",
    "    query = qa['question']\n",
    "    query = query.capitalize()\n",
    "    (start, end) = qa['answer']\n",
    "    answer_text = qa['context'][start:end]\n",
    "    context_list = qa['context'].split('. ')\n",
    "    for sentence in context_list:\n",
    "        if answer_text in sentence:\n",
    "            break\n",
    "    if 'What' in query:\n",
    "        what['count'] += 1\n",
    "        what['list'].append(sentence)\n",
    "    elif 'Why' in query:\n",
    "        why['count'] += 1\n",
    "        why['list'].append(sentence)\n",
    "    elif 'Where' in query:\n",
    "        where['count'] += 1\n",
    "        where['list'].append(sentence)\n",
    "    elif 'When' in query:\n",
    "        when['count'] += 1\n",
    "        when['list'].append(sentence)\n",
    "    elif 'How much' in query:\n",
    "        how_much['count'] += 1\n",
    "        how_much['list'].append(sentence)\n",
    "    elif 'How many' in query:\n",
    "        how_many['count'] += 1\n",
    "        how_many['list'].append(sentence)\n",
    "    elif 'How' in query:\n",
    "        how['count'] += 1\n",
    "        how['list'].append(sentence)\n",
    "    elif 'Which' in query:\n",
    "        which['count'] += 1\n",
    "        which['list'].append(sentence)\n",
    "    elif 'Who' in query:\n",
    "        who['count'] += 1\n",
    "        who['list'].append(sentence)\n",
    "    else:\n",
    "        other['count'] += 1\n",
    "        other['list'].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d99a75e-8186-4267-bf11-203c8a644a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of type of questions:  10\n"
     ]
    }
   ],
   "source": [
    "why_wordcount = sort_wordCounter(count_words(why['list']))\n",
    "what_wordcount = sort_wordCounter(count_words(what['list']))\n",
    "where_wordcount = sort_wordCounter(count_words(where['list']))\n",
    "when_wordcount = sort_wordCounter(count_words(when['list']))\n",
    "howmuch_wordcount = sort_wordCounter(count_words(how_much['list']))\n",
    "howmany_wordcount = sort_wordCounter(count_words(how_many['list']))\n",
    "how_wordcount = sort_wordCounter(count_words(how['list']))\n",
    "which_wordcount = sort_wordCounter(count_words(which['list']))\n",
    "who_wordcount = sort_wordCounter(count_words(who['list']))\n",
    "other_wordcount = sort_wordCounter(count_words(other['list']))\n",
    "\n",
    "wordcount_list = [why_wordcount, what_wordcount, where_wordcount, when_wordcount, howmuch_wordcount, howmany_wordcount, how_wordcount, which_wordcount, who_wordcount, other_wordcount]\n",
    "print('Num of type of questions: ', len(wordcount_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e48683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(wordCount,qtype, wordcount_list=wordcount_list):\n",
    "    pmi_dict = {}\n",
    "    for word in wordCount:\n",
    "        # word occurance more than quenstion number, the word must be a very common word,\n",
    "        # like 'the', 'a', 'it', etc. These words does not help answering qestions\n",
    "\n",
    "        if wordCount[word]>qtype['count']:\n",
    "            continue\n",
    "        # word occurance less than 5% quenstion number\n",
    "        # it should be a random occurance, we consider there is no relation between such word and the question\n",
    "        if wordCount[word]<qtype['count']/20:\n",
    "            break\n",
    "        else:\n",
    "            num_word = 0\n",
    "            vocab_num = 0\n",
    "            for one_wordcount in wordcount_list:\n",
    "                if word in one_wordcount:\n",
    "                    num_word += one_wordcount[word]\n",
    "                vocab_num += len(one_wordcount)\n",
    "            p_word = num_word / vocab_num\n",
    "            conditional_p_word = wordCount[word]/len(wordCount)\n",
    "            \n",
    "            pmi_dict[word] = np.log(conditional_p_word/p_word)\n",
    "    \n",
    "    return pmi_dict\n",
    "\n",
    "def get_top50_prompt_words(why_wordcount,qtype):    \n",
    "    #input: word count dict\n",
    "    #output: top 50 prompt word list of corresponds to the word count\n",
    "    why_word_pmiDict = pmi(why_wordcount, qtype) \n",
    "    sort_why_pmi = sort_wordCounter(why_word_pmiDict)\n",
    "    top50_whyPromptWords_list = list(sort_why_pmi)[0:10]\n",
    "    return top50_whyPromptWords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ab273b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top50_whyPromptWords_list = get_top50_prompt_words(why_wordcount, why)\n",
    "top50_whatPromptWords_list = get_top50_prompt_words(what_wordcount, what)\n",
    "top50_wherePromptWords_list = get_top50_prompt_words(where_wordcount, where)\n",
    "top50_whenPromptWords_list = get_top50_prompt_words(when_wordcount, when)\n",
    "top50_howmuchPromptWords_list = get_top50_prompt_words(howmuch_wordcount, how_much)\n",
    "top50_howmanyPromptWords_list = get_top50_prompt_words(howmany_wordcount, how_many)\n",
    "top50_howPromptWords_list = get_top50_prompt_words(how_wordcount, how)\n",
    "top50_whichPromptWords_list = get_top50_prompt_words(which_wordcount, which)\n",
    "top50_whoPromptWords_list = get_top50_prompt_words(who_wordcount, who)\n",
    "top50_otherPromptWords_list = get_top50_prompt_words(other_wordcount, other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "797af281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Qtype_template(qa_dataset):\n",
    "    # Random given prompt word for Qtype\n",
    "\n",
    "    for qa in qa_dataset:\n",
    "        query = qa['question']\n",
    "        (start, end) = qa['answer']\n",
    "        answer_text = qa['context'][start:end]\n",
    "        context_list = qa['context'].split('. ')\n",
    "        for sentence in context_list:\n",
    "            if answer_text in sentence:\n",
    "                break\n",
    "    \n",
    "    for qa in qa_dataset:\n",
    "        query = qa['question']\n",
    "        query = query.capitalize()\n",
    "        (start, end) = qa['answer']\n",
    "        answer_text = qa['context'][start:end]\n",
    "        context_list = qa['context'].split('. ')\n",
    "        for sentence in context_list:\n",
    "            if answer_text in sentence:\n",
    "                break\n",
    "\n",
    "        # question type\n",
    "\n",
    "        if 'What' in query:\n",
    "             template_01 = 'To answer a \"What\" question, you need to look for \"{}\"'.format(top50_whatPromptWords_list[random.randint(0,len(top50_whatPromptWords_list)-1)]) \n",
    "        elif 'Why' in query:\n",
    "            template_01 = 'To answer a \"Why\" question, you need to look for \"{}\"'.format(top50_whyPromptWords_list[random.randint(0,len(top50_whyPromptWords_list)-1)]) \n",
    "        elif 'Where' in query:\n",
    "            template_01 = 'To answer a \"Where\" question, you need to look for \"{}\"'.format(top50_wherePromptWords_list[random.randint(0,len(top50_wherePromptWords_list)-1)]) \n",
    "        elif 'When' in query:\n",
    "            template_01 = 'To answer a \"When\" question, you need to look for \"{}\"'.format(top50_whenPromptWords_list[random.randint(0,len(top50_whenPromptWords_list)-1)]) \n",
    "        elif 'How much' in query:\n",
    "            template_01 = 'To answer a \"How much\" question, you need to look for \"{}\"'.format(top50_howmuchPromptWords_list[random.randint(0,len(top50_howmuchPromptWords_list)-1)]) \n",
    "        elif 'How many' in query:\n",
    "            template_01 = 'To answer a \"How many\" question, you need to look for \"{}\"'.format(top50_howmanyPromptWords_list[random.randint(0,len(top50_howmanyPromptWords_list)-1)]) \n",
    "        elif 'How' in query:\n",
    "            template_01 = 'To answer a \"How\" question, you need to look for \"{}\"'.format(top50_howPromptWords_list[random.randint(0,len(top50_howPromptWords_list)-1)]) \n",
    "        elif 'Which' in query:\n",
    "            template_01 = 'To answer a \"Which\" question, you need to look for \"{}\"'.format(top50_whichPromptWords_list[random.randint(0,len(top50_whichPromptWords_list)-1)]) \n",
    "        elif 'Who' in query:\n",
    "            template_01 = 'To answer a \"Who\" question, you need to look for \"{}\"'.format(top50_whoPromptWords_list[random.randint(0,len(top50_whoPromptWords_list)-1)]) \n",
    "        else:\n",
    "            template_01 = 'To answer a \"Random\" question, you need to look for \"{}\"'.format(top50_otherPromptWords_list[random.randint(0,len(top50_otherPromptWords_list)-1)]) \n",
    "\n",
    "\n",
    "        template = template_01\n",
    "    \n",
    "        qa['template'] = template\n",
    "    \n",
    "    return qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d75b7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['newsQA_test_3.json',\n",
       " 'newsQA_test_2.json',\n",
       " 'newsQA_test_0.json',\n",
       " 'newsQA_test_1.json',\n",
       " 'newsQA_test_4.json']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_squad =  '../../data/NewsQA/testsets/'\n",
    "Qtype_squad = '../../data/NewsQA/Qtype_testsets/'\n",
    "filelist = os.listdir(original_squad)\n",
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "795783bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = json.load(open('../../data/NewsQA/processed_newsQA.json'))\n",
    "# withPrompt = generate_Qtype_template(f)\n",
    "# json.dump(withPrompt, open('../../data/NewsQA/processed_Qtype_newsQA.json', 'w', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dcf4f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in filelist:\n",
    "    f = json.load(open(os.path.join(original_squad, file)))\n",
    "    withPrompt = generate_Qtype_template(f)\n",
    "    json.dump(withPrompt, open(os.path.join(Qtype_squad, file), 'w', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8521633f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': \"KATHMANDU, Nepal (CNN) -- Monsoon flooding has left 50,000 people homeless in Nepal and killed at least 74 people in northern India, according to officials.\\n\\n\\n\\nMonsoon rain has bought heavy flooding to southern Nepal, pictured, and northern India.\\n\\n\\n\\nThe Nepalese army Thursday used helicopters to rescue residents in southeastern Nepal who were stranded on treetops and roofs after monsoon flooding forced more than 50,000 people from their homes.\\n\\n\\n\\nIn northern India monsoon flooding destroyed mud huts and killed at least 74 people, officials said. The deaths were in the state of Uttar Pradesh.\\n\\n\\n\\nThe latest casualties bring to more than 300 the number of people killed in India since the start of this year's monsoon season.\\n\\n\\n\\nNepalese officials said at least four southern villages were inundated with water after the embankment on the Saptakoshi River burst Monday.\\n\\n\\n\\nSome 40,000 people were now living in government-run relief camps; others had gone to other villages to stay with relatives, authorities said.\\n\\n\\n\\nNepalese Home Ministry officials said there were unconfirmed reports of deaths related to the flooding but authorities could not reach the affected areas to verify those reports.\\n\\n\\n\\nPolice and army troops had been using elephants, boats and helicopters to rescue stranded villagers in Sunsari district, about 400 km (249 miles) southeast of Kathmandu, since Monday.\\n\\n\\n\\nNepal amd India are in the midst of the annual monsoon season.\",\n",
       " 'question': 'Where have as many as 50,000 been forced from their homes?',\n",
       " 'answer': [78, 85],\n",
       " 'answer_text': 'Nepal ',\n",
       " 'start_position': 12,\n",
       " 'end_position': 12,\n",
       " 'template': 'To answer a \"Where\" question, you need to look for \"where\"'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_qtype = json.load(open(os.path.join(Qtype_squad, 'newsQA_test_3.json'), 'r', encoding='utf-8'))\n",
    "dev_qtype[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08092cb7",
   "metadata": {},
   "source": [
    "### Add phrase template to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7b38c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrase_template_for_domain(qa_dataset):\n",
    "    pattern = \"\"\"\n",
    "        NP: {++}\n",
    "        NPs: {++}\n",
    "        Ns: {++}\n",
    "        \"\"\"\n",
    "    pattenParse = nltk.RegexpParser(pattern)\n",
    "\n",
    "    # count=0\n",
    "    for qa in qa_dataset:\n",
    "        query = qa['question']\n",
    "        (start, end) = qa['answer']\n",
    "        answer_text = qa['context'][start:end]\n",
    "        context_list = qa['context'].split('. ')\n",
    "\n",
    "        tokens = nltk.word_tokenize(query)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        cs = pattenParse.parse(tagged)\n",
    "        phrase_template = '\"'\n",
    "        word_template = '\"'\n",
    "        candidates = []\n",
    "        for i in cs:\n",
    "            if type(i) != tuple: \n",
    "                for word in i:\n",
    "                    phrase_template += word[0]+' ' \n",
    "                phrase_template += ', '\n",
    "                # break\n",
    "            elif i[1] == 'JJ' or i[1] == 'NN' or i[1] == 'NNS':\n",
    "                candidates.append(i[0])\n",
    "        phrase_template = phrase_template[:-3]+'\"'\n",
    "\n",
    "        if ', ' in phrase_template:\n",
    "            template_01 = phrase_template + ' are important phrases. '\n",
    "        elif ' ' in phrase_template:\n",
    "            template_01 = phrase_template + ' is an important phrase. '\n",
    "        elif len(phrase_template)>3:\n",
    "            template_01 = phrase_template + ' is an important word. '\n",
    "        else:\n",
    "            template_01 = 'There is no important phrase in this query. '\n",
    "            # template_01 = ' '\n",
    "\n",
    "        if len(candidates)>0:\n",
    "            for candi in range(len(candidates)):\n",
    "                word_template+=candidates[candi]+'\", '\n",
    "                # if candi >= 2:\n",
    "                #     break\n",
    "            word_template = word_template[:-2]\n",
    "\n",
    "            if template_01 == 'There is no important phrase in this query. ':\n",
    "                template_02 = 'But also pay attention to these words: ' + word_template\n",
    "            else:\n",
    "                template_02 = 'And also pay attention to these words: ' + word_template\n",
    "\n",
    "            template = template_01+template_02\n",
    "        else:\n",
    "            template = template_01\n",
    "\n",
    "        qa['template'] = template\n",
    "\n",
    "    return qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8bd4fb48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Illegal chunk pattern: {++}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/site-packages/nltk/chunk/regexp.py:379\u001b[0m, in \u001b[0;36mRegexpChunkRule.fromstring\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mif\u001b[39;00m rule[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m rule[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m}\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     \u001b[39mreturn\u001b[39;00m ChunkRule(rule[\u001b[39m1\u001b[39;49m:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], comment)\n\u001b[1;32m    380\u001b[0m \u001b[39melif\u001b[39;00m rule[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m}\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m rule[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/site-packages/nltk/chunk/regexp.py:420\u001b[0m, in \u001b[0;36mChunkRule.__init__\u001b[0;34m(self, tag_pattern, descr)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pattern \u001b[39m=\u001b[39m tag_pattern\n\u001b[0;32m--> 420\u001b[0m regexp \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49mcompile(\n\u001b[1;32m    421\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m(?P<chunk>\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    422\u001b[0m     \u001b[39m%\u001b[39;49m (tag_pattern2re_pattern(tag_pattern), ChunkString\u001b[39m.\u001b[39;49mIN_STRIP_PATTERN)\n\u001b[1;32m    423\u001b[0m )\n\u001b[1;32m    424\u001b[0m RegexpChunkRule\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, regexp, \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mg<chunk>}\u001b[39m\u001b[39m\"\u001b[39m, descr)\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/re.py:252\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mCompile a regular expression pattern, returning a Pattern object.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 252\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/re.py:304\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mfirst argument must be string or compiled pattern\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 304\u001b[0m p \u001b[39m=\u001b[39m sre_compile\u001b[39m.\u001b[39;49mcompile(pattern, flags)\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (flags \u001b[39m&\u001b[39m DEBUG):\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/sre_compile.py:788\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    787\u001b[0m     pattern \u001b[39m=\u001b[39m p\n\u001b[0;32m--> 788\u001b[0m     p \u001b[39m=\u001b[39m sre_parse\u001b[39m.\u001b[39;49mparse(p, flags)\n\u001b[1;32m    789\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/sre_parse.py:955\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m     p \u001b[39m=\u001b[39m _parse_sub(source, state, flags \u001b[39m&\u001b[39;49m SRE_FLAG_VERBOSE, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m    956\u001b[0m \u001b[39mexcept\u001b[39;00m Verbose:\n\u001b[1;32m    957\u001b[0m     \u001b[39m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[39;00m\n\u001b[1;32m    958\u001b[0m     \u001b[39m# on the safe side, we'll parse the whole thing again...\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/sre_parse.py:444\u001b[0m, in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m    445\u001b[0m                        \u001b[39mnot\u001b[39;49;00m nested \u001b[39mand\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m items))\n\u001b[1;32m    446\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/sre_parse.py:841\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    839\u001b[0m sub_verbose \u001b[39m=\u001b[39m ((verbose \u001b[39mor\u001b[39;00m (add_flags \u001b[39m&\u001b[39m SRE_FLAG_VERBOSE)) \u001b[39mand\u001b[39;00m\n\u001b[1;32m    840\u001b[0m                \u001b[39mnot\u001b[39;00m (del_flags \u001b[39m&\u001b[39m SRE_FLAG_VERBOSE))\n\u001b[0;32m--> 841\u001b[0m p \u001b[39m=\u001b[39m _parse_sub(source, state, sub_verbose, nested \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    842\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m source\u001b[39m.\u001b[39mmatch(\u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/sre_parse.py:444\u001b[0m, in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m    445\u001b[0m                        \u001b[39mnot\u001b[39;49;00m nested \u001b[39mand\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m items))\n\u001b[1;32m    446\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/sre_parse.py:669\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m item \u001b[39mor\u001b[39;00m item[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m AT:\n\u001b[0;32m--> 669\u001b[0m     \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mnothing to repeat\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    670\u001b[0m                        source\u001b[39m.\u001b[39mtell() \u001b[39m-\u001b[39m here \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(this))\n\u001b[1;32m    671\u001b[0m \u001b[39mif\u001b[39;00m item[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m _REPEATCODES:\n",
      "\u001b[0;31merror\u001b[0m: nothing to repeat at position 10",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [91], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m filelist:\n\u001b[1;32m      3\u001b[0m     f \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(original_squad, file)))\n\u001b[0;32m----> 4\u001b[0m     withPrompt \u001b[39m=\u001b[39m generate_phrase_template_for_domain(f)\n\u001b[1;32m      5\u001b[0m     json\u001b[39m.\u001b[39mdump(withPrompt, \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(Phrase_squad, file), \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn [90], line 7\u001b[0m, in \u001b[0;36mgenerate_phrase_template_for_domain\u001b[0;34m(qa_dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_phrase_template_for_domain\u001b[39m(qa_dataset):\n\u001b[1;32m      2\u001b[0m     pattern \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39m        NP: \u001b[39m\u001b[39m{\u001b[39m\u001b[39m++}\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39m        NPs: \u001b[39m\u001b[39m{\u001b[39m\u001b[39m++}\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39m        Ns: \u001b[39m\u001b[39m{\u001b[39m\u001b[39m++}\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     pattenParse \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mRegexpParser(pattern)\n\u001b[1;32m      9\u001b[0m     \u001b[39m# count=0\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39mfor\u001b[39;00m qa \u001b[39min\u001b[39;00m qa_dataset:\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/site-packages/nltk/chunk/regexp.py:1198\u001b[0m, in \u001b[0;36mRegexpParser.__init__\u001b[0;34m(self, grammar, root_label, loop, trace)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop \u001b[39m=\u001b[39m loop\n\u001b[1;32m   1197\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(grammar, \u001b[39mstr\u001b[39m):\n\u001b[0;32m-> 1198\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_grammar(grammar, root_label, trace)\n\u001b[1;32m   1199\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1200\u001b[0m     \u001b[39m# Make sur the grammar looks like it has the right type:\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m     type_err \u001b[39m=\u001b[39m (\n\u001b[1;32m   1202\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected string or list of RegexpChunkParsers \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfor the grammar.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1203\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/site-packages/nltk/chunk/regexp.py:1238\u001b[0m, in \u001b[0;36mRegexpParser._read_grammar\u001b[0;34m(self, grammar, root_label, trace)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m     \u001b[39m# Add the rule\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m     rules\u001b[39m.\u001b[39mappend(RegexpChunkRule\u001b[39m.\u001b[39;49mfromstring(line))\n\u001b[1;32m   1240\u001b[0m \u001b[39m# Record the final stage\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_stage(rules, lhs, root_label, trace)\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/site-packages/nltk/chunk/regexp.py:394\u001b[0m, in \u001b[0;36mRegexpChunkRule.fromstring\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIllegal chunk pattern: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m rule)\n\u001b[1;32m    393\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, re\u001b[39m.\u001b[39merror) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 394\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIllegal chunk pattern: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m rule) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Illegal chunk pattern: {++}"
     ]
    }
   ],
   "source": [
    "Phrase_squad = '../../data/SQuAD_V2/Phrase_SQUAD/'\n",
    "for file in filelist:\n",
    "    f = json.load(open(os.path.join(original_squad, file)))\n",
    "    withPrompt = generate_phrase_template_for_domain(f)\n",
    "    json.dump(withPrompt, open(os.path.join(Phrase_squad, file), 'w', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8940f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('YJ_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7475eaa5ebb5e7461f5c86523f73115630374cb51a80189e2a305ed529ddb5a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
