{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d389cc86-14f2-4ab9-bdee-27e88e11e3da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /home/yingjie_niu/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "# import svgling\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download([\n",
    "    \"names\",\n",
    "    \"stopwords\",\n",
    "    \"state_union\",\n",
    "    \"twitter_samples\",\n",
    "    \"movie_reviews\",\n",
    "    \"averaged_perceptron_tagger\",\n",
    "    \"vader_lexicon\",\n",
    "    \"punkt\",\n",
    "    \"maxent_ne_chunker\",\n",
    "    \"words\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b7e06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 432,\n",
       " 'context': \"The global restaurant sector has come under pressure as several markets have restricted service in an effort to curb the spread of the coronavirus. Restrictions vary by geography, but even in markets where carry-out and drive-thru orders are still permitted, we expect uneven guest counts into 2021. Nevertheless, we believe investors should prioritize those players that possess the scale to be more aggressive on pricing near term (value-oriented players tend to outperform during economic shocks), give their customers greater access through robust digital ordering, delivery, and drive-thru capabilities, and have healthy balance sheets (both at the corporate and franchisee level) when looking for restaurant industry investment opportunities. While we believe COVID-19 disruptions may result in some restructuring situations and restaurant closures in 2020 among RBI's franchisee base, the operational strategies behind its core brands--Burger King, Tim Hortons, and Popeyes Louisiana Kitchen--should help position the company for longer-term market share gains. These include a focus on brand awareness and guest experience investments; increased use of technology as a customer acquisition, throughput, and marketing tool (including a widespread drive-thru modernization plan across each of its brands); international expansion; and channel diversification.\",\n",
       " 'question': 'Why The global restaurant sector has come under pressure?',\n",
       " 'answer': [56, 146],\n",
       " 'timestamp': '20201206'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qa_dataset = testset0+testset1+testset2+testset3+testset4\n",
    "qa_dataset = json.load(open('../../data/CausalQA/causalqa.json', 'r', encoding='utf-8'))\n",
    "qa_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d9d19",
   "metadata": {},
   "source": [
    "### Question Type Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e91c258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of qa pairs:  24462\n"
     ]
    }
   ],
   "source": [
    "print('Number of qa pairs: ', len(qa_dataset))\n",
    "question_list = []\n",
    "for qa in qa_dataset: \n",
    "    question_list.append(qa['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3d4caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why:  7705 What:  5241 What will happen 7694 Why useful 3822 Other 0\n",
      "24462\n",
      "24462\n"
     ]
    }
   ],
   "source": [
    "what = {'count':0, 'list':[]}\n",
    "why = {'count':0, 'list':[]}\n",
    "what_willhappen = {'count':0, 'list':[]}\n",
    "why_useful = {'count':0, 'list':[]}\n",
    "other = {'count':0, 'list':[]}\n",
    "\n",
    "for query in question_list:\n",
    "    query = query.capitalize()\n",
    "    if 'What will happen' in query:\n",
    "        what_willhappen['count'] += 1\n",
    "        what_willhappen['list'].append(query)\n",
    "    elif 'What' in query:\n",
    "        what['count'] += 1\n",
    "        what['list'].append(query)\n",
    "    elif ('Why' in query) and ('useful' in query):\n",
    "        why_useful['count'] += 1\n",
    "        why_useful['list'].append(query)\n",
    "    elif 'Why' in query:\n",
    "        why['count'] += 1\n",
    "        why['list'].append(query)\n",
    "    else:\n",
    "        other['count'] += 1\n",
    "        other['list'].append(query)\n",
    "print('Why: ',why['count'], 'What: ',what['count'], 'What will happen',what_willhappen['count'], 'Why useful',why_useful['count'], 'Other', other['count'])\n",
    "print(why['count']+what['count']+what_willhappen['count']+why_useful['count']+other['count'])\n",
    "print(len(question_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d919086f",
   "metadata": {},
   "source": [
    "### Add Qtype template to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc5b8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(dataset):\n",
    "    words = {}\n",
    "    for i in range(len(dataset)):\n",
    "        for word in dataset[i].split():\n",
    "            if word not in words:\n",
    "                words[word]=1\n",
    "            else:\n",
    "                words[word]+=1\n",
    "    return words\n",
    "\n",
    "def sort_wordCounter(words_dict):\n",
    "    # input: a dict of words, key:word, value:times of word shown in the context\n",
    "    # output: a list sorted by the value of dict\n",
    "    a = dict(sorted(words_dict.items(), key=lambda x:x[1], reverse=True))\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8406d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "what = {'count':0, 'list':[]}\n",
    "why = {'count':0, 'list':[]}\n",
    "what_willhappen = {'count':0, 'list':[]}\n",
    "why_useful = {'count':0, 'list':[]}\n",
    "other = {'count':0, 'list':[]}\n",
    "\n",
    "for qa in qa_dataset:\n",
    "    query = qa['question']\n",
    "    query = query.capitalize()\n",
    "    (start, end) = qa['answer']\n",
    "    answer_text = qa['context'][start:end]\n",
    "    context_list = qa['context'].split('. ')\n",
    "    for sentence in context_list:\n",
    "        if answer_text in sentence:\n",
    "            break\n",
    "    \n",
    "    if 'What will happen' in query:\n",
    "        what_willhappen['count'] += 1\n",
    "        what_willhappen['list'].append(sentence)\n",
    "    elif 'What' in query:\n",
    "        what['count'] += 1\n",
    "        what['list'].append(sentence)\n",
    "    elif ('Why' in query) and ('useful' in query):\n",
    "        why_useful['count'] += 1\n",
    "        why_useful['list'].append(sentence)\n",
    "    elif 'Why' in query:\n",
    "        why['count'] += 1\n",
    "        why['list'].append(sentence)\n",
    "    else:\n",
    "        other['count'] += 1\n",
    "        other['list'].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d99a75e-8186-4267-bf11-203c8a644a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of type of questions:  5\n"
     ]
    }
   ],
   "source": [
    "why_wordcount = sort_wordCounter(count_words(why['list']))\n",
    "what_wordcount = sort_wordCounter(count_words(what['list']))\n",
    "whatwillhappen_wordcount = sort_wordCounter(count_words(what_willhappen['list']))\n",
    "whyuseful_wordcount = sort_wordCounter(count_words(why_useful['list']))\n",
    "other_wordcount = sort_wordCounter(count_words(other['list']))\n",
    "\n",
    "wordcount_list = [why_wordcount, what_wordcount, whatwillhappen_wordcount,whyuseful_wordcount, other_wordcount]\n",
    "print('Num of type of questions: ', len(wordcount_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1e48683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(wordCount,qtype, wordcount_list=wordcount_list):\n",
    "    pmi_dict = {}\n",
    "    for word in wordCount:\n",
    "        # word occurance more than quenstion number, the word must be a very common word,\n",
    "        # like 'the', 'a', 'it', etc. These words does not help answering qestions\n",
    "\n",
    "        if wordCount[word]>qtype['count']:\n",
    "            continue\n",
    "        # word occurance less than 5% quenstion number\n",
    "        # it should be a random occurance, we consider there is no relation between such word and the question\n",
    "        if wordCount[word]<qtype['count']/20:\n",
    "            break\n",
    "        else:\n",
    "            num_word = 0\n",
    "            vocab_num = 0\n",
    "            for one_wordcount in wordcount_list:\n",
    "                if word in one_wordcount:\n",
    "                    num_word += one_wordcount[word]\n",
    "                vocab_num += len(one_wordcount)\n",
    "            p_word = num_word / vocab_num\n",
    "            conditional_p_word = wordCount[word]/len(wordCount)\n",
    "            \n",
    "            pmi_dict[word] = np.log(conditional_p_word/p_word)\n",
    "    \n",
    "    return pmi_dict\n",
    "\n",
    "def get_top50_prompt_words(why_wordcount,qtype):    \n",
    "    #input: word count dict\n",
    "    #output: top 50 prompt word list of corresponds to the word count\n",
    "    why_word_pmiDict = pmi(why_wordcount, qtype) \n",
    "    sort_why_pmi = sort_wordCounter(why_word_pmiDict)\n",
    "    top50_whyPromptWords_list = list(sort_why_pmi)[0:10]\n",
    "    return top50_whyPromptWords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ab273b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top50_whyPromptWords_list = get_top50_prompt_words(why_wordcount, why)\n",
    "top50_whatPromptWords_list = get_top50_prompt_words(what_wordcount, what)\n",
    "top50_whatwillhappenPromptWords_list = get_top50_prompt_words(whatwillhappen_wordcount, what_willhappen)\n",
    "top50_whyusefulPromptWords_list = get_top50_prompt_words(whyuseful_wordcount, why_useful)\n",
    "top50_otherPromptWords_list = get_top50_prompt_words(other_wordcount, other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a88b41f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['through',\n",
       " 'should',\n",
       " 'firm',\n",
       " 'into',\n",
       " 'its',\n",
       " 'believe',\n",
       " 'it',\n",
       " 'cost',\n",
       " 'can',\n",
       " 'customers']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top50_whyusefulPromptWords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "797af281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Qtype_template(qa_dataset):\n",
    "    # Random given prompt word for Qtype\n",
    "\n",
    "    for qa in qa_dataset:\n",
    "        query = qa['question']\n",
    "        (start, end) = qa['answer']\n",
    "        answer_text = qa['context'][start:end]\n",
    "        context_list = qa['context'].split('. ')\n",
    "        for sentence in context_list:\n",
    "            if answer_text in sentence:\n",
    "                break\n",
    "    \n",
    "    for qa in qa_dataset:\n",
    "        query = qa['question']\n",
    "        query = query.capitalize()\n",
    "        (start, end) = qa['answer']\n",
    "        answer_text = qa['context'][start:end]\n",
    "        context_list = qa['context'].split('. ')\n",
    "        for sentence in context_list:\n",
    "            if answer_text in sentence:\n",
    "                break\n",
    "\n",
    "        # question type\n",
    "            \n",
    "        if \"What will happen\" in query:\n",
    "            template_01 = 'To answer a \"What will happen\" question, you need to look for \"{}\"'.format(top50_whatwillhappenPromptWords_list[random.randint(0,len(top50_whatwillhappenPromptWords_list)-1)])   \n",
    "        elif 'What' in query:\n",
    "            template_01 = 'To answer a \"What\" question, you need to look for \"{}\"'.format(top50_whatPromptWords_list[random.randint(0,len(top50_whatPromptWords_list)-1)])   \n",
    "        elif ('Why' in query) and ('useful' in query):\n",
    "            template_01 = 'To answer a \"Why useful\" question, you need to look for \"{}\"'.format(top50_whyusefulPromptWords_list[random.randint(0,len(top50_whyusefulPromptWords_list)-1)])   \n",
    "        elif 'Why' in query:\n",
    "            template_01 = 'To answer a \"Why\" question, you need to look for \"{}\"'.format(top50_whyPromptWords_list[random.randint(0,len(top50_whyPromptWords_list)-1)])   \n",
    "\n",
    "\n",
    "        template = template_01\n",
    "    \n",
    "        qa['template'] = template\n",
    "    \n",
    "    return qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "795783bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = json.load(open('../../data/CausalQA/causalqa.json'))\n",
    "withPrompt = generate_Qtype_template(f)\n",
    "json.dump(withPrompt, open('../../data/CausalQA/causalQA_Qtype.json', 'w', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8521633f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 432,\n",
       " 'context': \"The global restaurant sector has come under pressure as several markets have restricted service in an effort to curb the spread of the coronavirus. Restrictions vary by geography, but even in markets where carry-out and drive-thru orders are still permitted, we expect uneven guest counts into 2021. Nevertheless, we believe investors should prioritize those players that possess the scale to be more aggressive on pricing near term (value-oriented players tend to outperform during economic shocks), give their customers greater access through robust digital ordering, delivery, and drive-thru capabilities, and have healthy balance sheets (both at the corporate and franchisee level) when looking for restaurant industry investment opportunities. While we believe COVID-19 disruptions may result in some restructuring situations and restaurant closures in 2020 among RBI's franchisee base, the operational strategies behind its core brands--Burger King, Tim Hortons, and Popeyes Louisiana Kitchen--should help position the company for longer-term market share gains. These include a focus on brand awareness and guest experience investments; increased use of technology as a customer acquisition, throughput, and marketing tool (including a widespread drive-thru modernization plan across each of its brands); international expansion; and channel diversification.\",\n",
       " 'question': 'Why The global restaurant sector has come under pressure?',\n",
       " 'answer': [56, 146],\n",
       " 'timestamp': '20201206',\n",
       " 'template': 'To answer a \"Why\" question, you need to look for \"would\"',\n",
       " 'start_position': 10,\n",
       " 'end_position': 27}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = json.load(open('../../data/CausalQA/causalQA_Qtype.json', 'r', encoding='utf-8'))\n",
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "413db84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_len = int(len(sample)/10)\n",
    "train = sample[:-2*dev_len]\n",
    "dev = sample[-2*dev_len:-1*dev_len]\n",
    "test = sample[-1*dev_len:]\n",
    "json.dump(train, open('../../data/CausalQA/Qtype_causal/causal_train.json', 'w', encoding='utf-8'))\n",
    "json.dump(dev, open('../../data/CausalQA/Qtype_causal/causal_dev.json', 'w', encoding='utf-8'))\n",
    "json.dump(test, open('../../data/CausalQA/Qtype_causal/causal_test.json', 'w', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "311347d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 432,\n",
       " 'context': \"The global restaurant sector has come under pressure as several markets have restricted service in an effort to curb the spread of the coronavirus. Restrictions vary by geography, but even in markets where carry-out and drive-thru orders are still permitted, we expect uneven guest counts into 2021. Nevertheless, we believe investors should prioritize those players that possess the scale to be more aggressive on pricing near term (value-oriented players tend to outperform during economic shocks), give their customers greater access through robust digital ordering, delivery, and drive-thru capabilities, and have healthy balance sheets (both at the corporate and franchisee level) when looking for restaurant industry investment opportunities. While we believe COVID-19 disruptions may result in some restructuring situations and restaurant closures in 2020 among RBI's franchisee base, the operational strategies behind its core brands--Burger King, Tim Hortons, and Popeyes Louisiana Kitchen--should help position the company for longer-term market share gains. These include a focus on brand awareness and guest experience investments; increased use of technology as a customer acquisition, throughput, and marketing tool (including a widespread drive-thru modernization plan across each of its brands); international expansion; and channel diversification.\",\n",
       " 'question': 'Why The global restaurant sector has come under pressure?',\n",
       " 'answer': [56, 146],\n",
       " 'timestamp': '20201206',\n",
       " 'start_position': 10,\n",
       " 'end_position': 27}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = json.load(open('../../data/CausalQA/original/causal_train.json', 'r', encoding='utf-8'))\n",
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f22d35e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = json.load(open('../../data/CausalQA/causalQA.json', 'r', encoding='utf-8'))\n",
    "dev_len = int(len(sample)/10)\n",
    "train = sample[:-2*dev_len]\n",
    "dev = sample[-2*dev_len:-1*dev_len]\n",
    "test = sample[-1*dev_len:]\n",
    "json.dump(train, open('../../data/CausalQA/original/causal_train.json', 'w', encoding='utf-8'))\n",
    "json.dump(dev, open('../../data/CausalQA/original/causal_dev.json', 'w', encoding='utf-8'))\n",
    "json.dump(test, open('../../data/CausalQA/original/causal_test.json', 'w', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08092cb7",
   "metadata": {},
   "source": [
    "### Add phrase template to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7b38c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrase_template_for_domain(qa_dataset):\n",
    "    pattern = \"\"\"\n",
    "        NP: {++}\n",
    "        NPs: {++}\n",
    "        Ns: {++}\n",
    "        \"\"\"\n",
    "    pattenParse = nltk.RegexpParser(pattern)\n",
    "\n",
    "    # count=0\n",
    "    for qa in qa_dataset:\n",
    "        query = qa['question']\n",
    "        (start, end) = qa['answer']\n",
    "        answer_text = qa['context'][start:end]\n",
    "        context_list = qa['context'].split('. ')\n",
    "\n",
    "        tokens = nltk.word_tokenize(query)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        cs = pattenParse.parse(tagged)\n",
    "        phrase_template = '\"'\n",
    "        word_template = '\"'\n",
    "        candidates = []\n",
    "        for i in cs:\n",
    "            if type(i) != tuple: \n",
    "                for word in i:\n",
    "                    phrase_template += word[0]+' ' \n",
    "                phrase_template += ', '\n",
    "                # break\n",
    "            elif i[1] == 'JJ' or i[1] == 'NN' or i[1] == 'NNS':\n",
    "                candidates.append(i[0])\n",
    "        phrase_template = phrase_template[:-3]+'\"'\n",
    "\n",
    "        if ', ' in phrase_template:\n",
    "            template_01 = phrase_template + ' are important phrases. '\n",
    "        elif ' ' in phrase_template:\n",
    "            template_01 = phrase_template + ' is an important phrase. '\n",
    "        elif len(phrase_template)>3:\n",
    "            template_01 = phrase_template + ' is an important word. '\n",
    "        else:\n",
    "            template_01 = 'There is no important phrase in this query. '\n",
    "            # template_01 = ' '\n",
    "\n",
    "        if len(candidates)>0:\n",
    "            for candi in range(len(candidates)):\n",
    "                word_template+=candidates[candi]+'\", '\n",
    "                # if candi >= 2:\n",
    "                #     break\n",
    "            word_template = word_template[:-2]\n",
    "\n",
    "            if template_01 == 'There is no important phrase in this query. ':\n",
    "                template_02 = 'But also pay attention to these words: ' + word_template\n",
    "            else:\n",
    "                template_02 = 'And also pay attention to these words: ' + word_template\n",
    "\n",
    "            template = template_01+template_02\n",
    "        else:\n",
    "            template = template_01\n",
    "\n",
    "        qa['template'] = template\n",
    "\n",
    "    return qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8bd4fb48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Illegal chunk pattern: {++}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/site-packages/nltk/chunk/regexp.py:379\u001b[0m, in \u001b[0;36mRegexpChunkRule.fromstring\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mif\u001b[39;00m rule[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m rule[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m}\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     \u001b[39mreturn\u001b[39;00m ChunkRule(rule[\u001b[39m1\u001b[39;49m:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], comment)\n\u001b[1;32m    380\u001b[0m \u001b[39melif\u001b[39;00m rule[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m}\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m rule[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/site-packages/nltk/chunk/regexp.py:420\u001b[0m, in \u001b[0;36mChunkRule.__init__\u001b[0;34m(self, tag_pattern, descr)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pattern \u001b[39m=\u001b[39m tag_pattern\n\u001b[0;32m--> 420\u001b[0m regexp \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49mcompile(\n\u001b[1;32m    421\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m(?P<chunk>\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    422\u001b[0m     \u001b[39m%\u001b[39;49m (tag_pattern2re_pattern(tag_pattern), ChunkString\u001b[39m.\u001b[39;49mIN_STRIP_PATTERN)\n\u001b[1;32m    423\u001b[0m )\n\u001b[1;32m    424\u001b[0m RegexpChunkRule\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, regexp, \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mg<chunk>}\u001b[39m\u001b[39m\"\u001b[39m, descr)\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/re.py:252\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mCompile a regular expression pattern, returning a Pattern object.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 252\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/re.py:304\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mfirst argument must be string or compiled pattern\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 304\u001b[0m p \u001b[39m=\u001b[39m sre_compile\u001b[39m.\u001b[39;49mcompile(pattern, flags)\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (flags \u001b[39m&\u001b[39m DEBUG):\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/sre_compile.py:788\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    787\u001b[0m     pattern \u001b[39m=\u001b[39m p\n\u001b[0;32m--> 788\u001b[0m     p \u001b[39m=\u001b[39m sre_parse\u001b[39m.\u001b[39;49mparse(p, flags)\n\u001b[1;32m    789\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/sre_parse.py:955\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m     p \u001b[39m=\u001b[39m _parse_sub(source, state, flags \u001b[39m&\u001b[39;49m SRE_FLAG_VERBOSE, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m    956\u001b[0m \u001b[39mexcept\u001b[39;00m Verbose:\n\u001b[1;32m    957\u001b[0m     \u001b[39m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[39;00m\n\u001b[1;32m    958\u001b[0m     \u001b[39m# on the safe side, we'll parse the whole thing again...\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/sre_parse.py:444\u001b[0m, in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m    445\u001b[0m                        \u001b[39mnot\u001b[39;49;00m nested \u001b[39mand\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m items))\n\u001b[1;32m    446\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/sre_parse.py:841\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    839\u001b[0m sub_verbose \u001b[39m=\u001b[39m ((verbose \u001b[39mor\u001b[39;00m (add_flags \u001b[39m&\u001b[39m SRE_FLAG_VERBOSE)) \u001b[39mand\u001b[39;00m\n\u001b[1;32m    840\u001b[0m                \u001b[39mnot\u001b[39;00m (del_flags \u001b[39m&\u001b[39m SRE_FLAG_VERBOSE))\n\u001b[0;32m--> 841\u001b[0m p \u001b[39m=\u001b[39m _parse_sub(source, state, sub_verbose, nested \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    842\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m source\u001b[39m.\u001b[39mmatch(\u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/sre_parse.py:444\u001b[0m, in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m    445\u001b[0m                        \u001b[39mnot\u001b[39;49;00m nested \u001b[39mand\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m items))\n\u001b[1;32m    446\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/sre_parse.py:669\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m item \u001b[39mor\u001b[39;00m item[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m AT:\n\u001b[0;32m--> 669\u001b[0m     \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mnothing to repeat\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    670\u001b[0m                        source\u001b[39m.\u001b[39mtell() \u001b[39m-\u001b[39m here \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(this))\n\u001b[1;32m    671\u001b[0m \u001b[39mif\u001b[39;00m item[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m _REPEATCODES:\n",
      "\u001b[0;31merror\u001b[0m: nothing to repeat at position 10",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [91], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m filelist:\n\u001b[1;32m      3\u001b[0m     f \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(original_squad, file)))\n\u001b[0;32m----> 4\u001b[0m     withPrompt \u001b[39m=\u001b[39m generate_phrase_template_for_domain(f)\n\u001b[1;32m      5\u001b[0m     json\u001b[39m.\u001b[39mdump(withPrompt, \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(Phrase_squad, file), \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn [90], line 7\u001b[0m, in \u001b[0;36mgenerate_phrase_template_for_domain\u001b[0;34m(qa_dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_phrase_template_for_domain\u001b[39m(qa_dataset):\n\u001b[1;32m      2\u001b[0m     pattern \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39m        NP: \u001b[39m\u001b[39m{\u001b[39m\u001b[39m++}\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39m        NPs: \u001b[39m\u001b[39m{\u001b[39m\u001b[39m++}\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39m        Ns: \u001b[39m\u001b[39m{\u001b[39m\u001b[39m++}\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     pattenParse \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mRegexpParser(pattern)\n\u001b[1;32m      9\u001b[0m     \u001b[39m# count=0\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39mfor\u001b[39;00m qa \u001b[39min\u001b[39;00m qa_dataset:\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/site-packages/nltk/chunk/regexp.py:1198\u001b[0m, in \u001b[0;36mRegexpParser.__init__\u001b[0;34m(self, grammar, root_label, loop, trace)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loop \u001b[39m=\u001b[39m loop\n\u001b[1;32m   1197\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(grammar, \u001b[39mstr\u001b[39m):\n\u001b[0;32m-> 1198\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_grammar(grammar, root_label, trace)\n\u001b[1;32m   1199\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1200\u001b[0m     \u001b[39m# Make sur the grammar looks like it has the right type:\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m     type_err \u001b[39m=\u001b[39m (\n\u001b[1;32m   1202\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected string or list of RegexpChunkParsers \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfor the grammar.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1203\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/site-packages/nltk/chunk/regexp.py:1238\u001b[0m, in \u001b[0;36mRegexpParser._read_grammar\u001b[0;34m(self, grammar, root_label, trace)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m     \u001b[39m# Add the rule\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m     rules\u001b[39m.\u001b[39mappend(RegexpChunkRule\u001b[39m.\u001b[39;49mfromstring(line))\n\u001b[1;32m   1240\u001b[0m \u001b[39m# Record the final stage\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_stage(rules, lhs, root_label, trace)\n",
      "File \u001b[0;32m~/anaconda3/envs/YJ_env/lib/python3.9/site-packages/nltk/chunk/regexp.py:394\u001b[0m, in \u001b[0;36mRegexpChunkRule.fromstring\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIllegal chunk pattern: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m rule)\n\u001b[1;32m    393\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, re\u001b[39m.\u001b[39merror) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 394\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIllegal chunk pattern: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m rule) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Illegal chunk pattern: {++}"
     ]
    }
   ],
   "source": [
    "Phrase_squad = '../../data/SQuAD_V2/Phrase_SQUAD/'\n",
    "for file in filelist:\n",
    "    f = json.load(open(os.path.join(original_squad, file)))\n",
    "    withPrompt = generate_phrase_template_for_domain(f)\n",
    "    json.dump(withPrompt, open(os.path.join(Phrase_squad, file), 'w', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b8aac73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00980392156862745"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.99*0.0001/(0.99*0.0001+0.01*0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d07b4d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6387"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8067-0.6527+0.4847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c35ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('YJ_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7475eaa5ebb5e7461f5c86523f73115630374cb51a80189e2a305ed529ddb5a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
